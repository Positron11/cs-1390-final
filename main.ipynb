{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author Profiling (Big 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by importing required libraries and installing NLP modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/aarush/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Load the labelled essays and emotion lexicon datasets as Pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                TEXT cEXT cNEU cAGR cCON cOPN\n",
      "0  Well, right now I just woke up from a mid-day ...    n    y    y    n    y\n",
      "1  Well, here we go with the stream of consciousn...    n    n    y    n    n\n",
      "2  An open keyboard and buttons to push. The thin...    n    y    n    y    y\n",
      "3  I can't believe it!  It's really happening!  M...    y    n    y    y    n\n",
      "4  Well, here I go with the good old stream of co...    y    n    y    n    y\n",
      "    Words  anger  anticipation  disgust  fear  joy  negative  positive  \\\n",
      "0   march      0             0        0     0    0         0         1   \n",
      "1  august      0             0        0     0    0         0         1   \n",
      "2     ago      0             0        0     0    0         0         0   \n",
      "3     mar      0             0        0     0    0         1         0   \n",
      "4     vie      0             0        0     0    0         0         0   \n",
      "\n",
      "   sadness  surprise  trust  Charged  \n",
      "0        0         0      0        1  \n",
      "1        0         0      0        1  \n",
      "2        0         0      0        0  \n",
      "3        0         0      0        1  \n",
      "4        0         0      0        0  \n"
     ]
    }
   ],
   "source": [
    "essays = pd.read_csv(\"data/essays.csv\").iloc[:, 1:]\n",
    "lexicon = pd.read_csv(\"data/lexicon.csv\")\n",
    "\n",
    "print(essays.head())\n",
    "print(lexicon.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a set of all the words in the emotion lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['forsaken', 'frenetic', 'follow', 'routine', 'boob', 'monologue', 'delicious', 'dismay', 'depositary', 'splendid', 'sentry', 'tuition', 'vinaigrette', 'poetical', 'hen', 'payment', 'huff', 'rope', 'sacrifices', 'gig', 'canary', 'disprove', 'haven', 'mitigation', 'foray', 'pious', 'back', 'alphabetical', 'letters', 'vocation', 'mixed', 'mosaic', 'buttress', 'chaise', 'prescriptive', 'upstart', 'lanky', 'fright', 'prescription', 'rollicking', 'ridiculous', 'accordance', 'gush', 'irrational', 'existent', 'method', 'legislative', 'fact', 'flanking', 'jog']\n"
     ]
    }
   ],
   "source": [
    "emotion_words = set(lexicon[\"Words\"])\n",
    "print(list(emotion_words)[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Sentences\n",
    "\n",
    "For each entry in the dataset, we will tokenize the text by sentence and drop any sentences that do not contain a word from the emotion lexicon - we deem these sentences to be too mild for drawing inferences from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sentences(text, emotion_words):\n",
    "\tsentences = sent_tokenize(text)\n",
    "\tfiltered_sentences = [sentence for sentence in sentences if any(word in emotion_words for word in word_tokenize(sentence))]\n",
    "\treturn ' '.join(filtered_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                TEXT cEXT cNEU cAGR cCON cOPN\n",
      "0  Well, right now I just woke up from a mid-day ...    n    y    y    n    y\n",
      "1  Well, here we go with the stream of consciousn...    n    n    y    n    n\n",
      "2  An open keyboard and buttons to push. The thin...    n    y    n    y    y\n",
      "3  It's really happening! My pulse is racing like...    y    n    y    y    n\n",
      "4  Well, here I go with the good old stream of co...    y    n    y    n    y\n"
     ]
    }
   ],
   "source": [
    "essays[\"TEXT\"] = essays[\"TEXT\"].apply(lambda x: filter_sentences(x, emotion_words))\n",
    "essays = essays[essays[\"TEXT\"].str.strip() != \"\"]\n",
    "\n",
    "print(essays.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### TF-IDF Vectorization\n",
    "\n",
    "Perform Term Frequency-Inverse Document Frequency vectorization on each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words=\"english\")\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(essays[\"TEXT\"]).toarray()\n",
    "\n",
    "print(tfidf_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Vectors\n",
    "\n",
    "Convert the emotion lexicon to a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 1 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "lexicon_dict = {\n",
    "    row[\"Words\"]: np.array(row[1:])  # Assuming emotions are in columns 1 onward\n",
    "    for _, row in lexicon.iterrows()\n",
    "}\n",
    "\n",
    "print(lexicon_dict[\"march\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a function to compute the cumulative sentiment vector of the text, and apply this function to the corpus to generate sentiment vector features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sentiment_vector(text, lexicon_dict):\n",
    "\ttokens = text.split()\n",
    "\t\n",
    "\tsentiment_vector = np.zeros(len(lexicon_dict[\"march\"]))\n",
    "\t\n",
    "\tfor token in tokens:\n",
    "\t\tif token in lexicon_dict:\n",
    "\t\t\tsentiment_vector += lexicon_dict[token].astype(float)\n",
    "\n",
    "\tif np.sum(sentiment_vector) > 0: sentiment_vector /= np.sum(sentiment_vector)\n",
    "\t\n",
    "\treturn sentiment_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3. 18.  0. ...  5. 12. 40.]\n",
      " [ 4. 20.  1. ... 10. 15. 53.]\n",
      " [ 6. 12.  5. ...  2. 11. 52.]\n",
      " ...\n",
      " [ 6. 10.  5. ...  5.  7. 38.]\n",
      " [ 9. 28.  4. ...  9. 23. 68.]\n",
      " [13. 31.  9. ... 22. 31. 72.]]\n"
     ]
    }
   ],
   "source": [
    "sentiment_vectors = np.array([compute_sentiment_vector(text, lexicon_dict) for text in essays[\"TEXT\"]])\n",
    "print(sentiment_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe Word Embeddings\n",
    "\n",
    "Function to load embeddings into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(filepath):\n",
    "\tembeddings = {}\n",
    "\t\n",
    "\twith open(filepath, 'r', encoding='utf-8') as f:\n",
    "\t\tfor line in f:\n",
    "\t\t\tvalues = line.split()\n",
    "\t\t\tword = values[0]\n",
    "\t\t\tvector = np.array(values[1:], dtype=\"float32\")\n",
    "\t\t\tembeddings[word] = vector\n",
    "\t\n",
    "\treturn embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load embeddings from the 6B token 200 vector dimension model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', ',', '.', 'of', 'to', 'and', 'in', 'a', '\"', \"'s\", 'for', '-', 'that', 'on', 'is', 'was', 'said', 'with', 'he', 'as', 'it', 'by', 'at', '(', ')', 'from', 'his', \"''\", '``', 'an', 'be', 'has', 'are', 'have', 'but', 'were', 'not', 'this', 'who', 'they', 'had', 'i', 'which', 'will', 'their', ':', 'or', 'its', 'one', 'after', 'new', 'been', 'also', 'we', 'would', 'two', 'more', \"'\", 'first', 'about', 'up', 'when', 'year', 'there', 'all', '--', 'out', 'she', 'other', 'people', \"n't\", 'her', 'percent', 'than', 'over', 'into', 'last', 'some', 'government', 'time', '$', 'you', 'years', 'if', 'no', 'world', 'can', 'three', 'do', ';', 'president', 'only', 'state', 'million', 'could', 'us', 'most', '_', 'against', 'u.s.']\n",
      "[-7.1549e-02  9.3459e-02  2.3738e-02 -9.0339e-02  5.6123e-02  3.2547e-01\n",
      " -3.9796e-01 -9.2139e-02  6.1181e-02 -1.8950e-01  1.3061e-01  1.4349e-01\n",
      "  1.1479e-02  3.8158e-01  5.4030e-01 -1.4088e-01  2.4315e-01  2.3036e-01\n",
      " -5.5339e-01  4.8154e-02  4.5662e-01  3.2338e+00  2.0199e-02  4.9019e-02\n",
      " -1.4132e-02  7.6017e-02 -1.1527e-01  2.0060e-01 -7.7657e-02  2.4328e-01\n",
      "  1.6368e-01 -3.4118e-01 -6.6070e-02  1.0152e-01  3.8232e-02 -1.7668e-01\n",
      " -8.8153e-01 -3.3895e-01 -3.5481e-02 -5.5095e-01 -1.6899e-02 -4.3982e-01\n",
      "  3.9004e-02  4.0447e-01 -2.5880e-01  6.4594e-01  2.6641e-01  2.8009e-01\n",
      " -2.4625e-02  6.3302e-01 -3.1700e-01  1.0271e-01  3.0886e-01  9.7792e-02\n",
      " -3.8227e-01  8.6552e-02  4.7075e-02  2.3511e-01 -3.2127e-01 -2.8538e-01\n",
      "  1.6670e-01 -4.9707e-03 -6.2714e-01 -2.4904e-01  2.9713e-01  1.4379e-01\n",
      " -1.2325e-01 -5.8178e-02 -1.0290e-03 -8.2126e-02  3.6935e-01 -5.8442e-04\n",
      "  3.4286e-01  2.8426e-01 -6.8599e-02  6.5747e-01 -2.9087e-02  1.6184e-01\n",
      "  7.3672e-02 -3.0343e-01  9.5733e-02 -5.2860e-01 -2.2898e-01  6.4079e-02\n",
      "  1.5218e-02  3.4921e-01 -4.3960e-01 -4.3983e-01  7.7515e-01 -8.7767e-01\n",
      " -8.7504e-02  3.9598e-01  6.2362e-01 -2.6211e-01 -3.0539e-01 -2.2964e-02\n",
      "  3.0567e-01  6.7660e-02  1.5383e-01 -1.1211e-01 -9.1540e-02  8.2562e-02\n",
      "  1.6897e-01 -3.2952e-02 -2.8775e-01 -2.2320e-01 -9.0426e-02  1.2407e+00\n",
      " -1.8244e-01 -7.5219e-03 -4.1388e-02 -1.1083e-02  7.8186e-02  3.8511e-01\n",
      "  2.3334e-01  1.4414e-01 -9.1070e-04 -2.6388e-01 -2.0481e-01  1.0099e-01\n",
      "  1.4076e-01  2.8834e-01 -4.5429e-02  3.7247e-01  1.3645e-01 -6.7457e-01\n",
      "  2.2786e-01  1.2599e-01  2.9091e-02  3.0428e-02 -1.3028e-01  1.9408e-01\n",
      "  4.9014e-01 -3.9121e-01 -7.5952e-02  7.4731e-02  1.8902e-01 -1.6922e-01\n",
      " -2.6019e-01 -3.9771e-02 -2.4153e-01  1.0875e-01  3.0434e-01  3.6009e-02\n",
      "  1.4264e+00  1.2759e-01 -7.3811e-02 -2.0418e-01  8.0016e-03  1.5381e-01\n",
      "  2.0223e-01  2.8274e-01  9.6206e-02 -3.3634e-01  5.0983e-01  3.2625e-01\n",
      " -2.6535e-01  3.7400e-01 -3.0388e-01 -4.0033e-01 -4.2910e-02 -6.7897e-02\n",
      " -2.9332e-01  1.0978e-01 -4.5365e-02  2.3222e-01 -3.1134e-01 -2.8983e-01\n",
      " -6.6687e-01  5.3097e-01  1.9461e-01  3.6670e-01  2.6185e-01 -6.5187e-01\n",
      "  1.0266e-01  1.1363e-01 -1.2953e-01 -6.8246e-01 -1.8751e-01  1.4760e-01\n",
      "  1.0765e+00 -2.2908e-01 -9.3435e-03 -2.0651e-01 -3.5225e-01 -2.6720e-01\n",
      " -3.4307e-03  2.5906e-01  2.1759e-01  6.6158e-01  1.2180e-01  1.9957e-01\n",
      " -2.0303e-01  3.4474e-01 -2.4328e-01  1.3139e-01 -8.8767e-03  3.3617e-01\n",
      "  3.0591e-02  2.5577e-01]\n"
     ]
    }
   ],
   "source": [
    "glove_embeddings = load_glove_embeddings(\"models/glove.6B.200d.txt\")\n",
    "print(list(glove_embeddings.keys())[:100])\n",
    "print(glove_embeddings[\"the\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a feature set of glove embeddings for each text by taking the mean of the vector corresponding to each word in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_glove_embedding(text, embeddings):\n",
    "\twords = word_tokenize(text)\n",
    "\t\n",
    "\tword_vectors = [embeddings[word] for word in words if word in embeddings]\n",
    "\t\n",
    "\tif word_vectors: return np.mean(word_vectors, axis=0)\n",
    "\telse: return np.zeros(50)  # match glove dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.17561968  0.21697745 -0.09997822 ...  0.17866525 -0.13878608\n",
      "   0.08554857]\n",
      " [ 0.19505993  0.24416132 -0.11080451 ...  0.15697885 -0.10536592\n",
      "   0.07854819]\n",
      " [ 0.15996172  0.21897827 -0.05512039 ...  0.16525042 -0.14665474\n",
      "   0.08982378]\n",
      " ...\n",
      " [ 0.14254373  0.23699257 -0.10872305 ...  0.15260881 -0.12059193\n",
      "   0.06254157]\n",
      " [ 0.20047392  0.22909714 -0.09466812 ...  0.147249   -0.13642329\n",
      "   0.10019936]\n",
      " [ 0.23306464  0.22932526 -0.08814638 ...  0.18132985 -0.1229706\n",
      "   0.09198   ]]\n"
     ]
    }
   ],
   "source": [
    "glove_embeddings = np.array([build_glove_embedding(text, glove_embeddings) for text in essays[\"TEXT\"]])\n",
    "print(glove_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Word Embeddings\n",
    "\n",
    "Load the BERT models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 13:07:50.312874: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the corpus in contextual batches of 32 and generate BERT word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings_batched(texts, batch_size=32):\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"tf\", padding=True, truncation=True, max_length=128)\n",
    "        outputs = bert_model(inputs[\"input_ids\"])\n",
    "        batch_embeddings = tf.reduce_mean(outputs.last_hidden_state, axis=1).numpy()\n",
    "        embeddings.append(batch_embeddings)\n",
    "    \n",
    "    return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00908303  0.01011803  0.18886453 ... -0.04336559  0.18032753\n",
      "   0.07847334]\n",
      " [ 0.03351337  0.13423398  0.15558639 ...  0.18691523 -0.03201956\n",
      "   0.09959432]\n",
      " [ 0.0451029   0.27323285  0.4640565  ... -0.02578287  0.2341059\n",
      "   0.08627795]\n",
      " ...\n",
      " [-0.13311921 -0.2170437   0.18040407 ... -0.00803733  0.22940615\n",
      "  -0.14817858]\n",
      " [ 0.25195867 -0.02217607  0.19356966 ... -0.09765543  0.07215264\n",
      "  -0.15513891]\n",
      " [ 0.07322188  0.07545072  0.20285086 ...  0.08045725  0.11770028\n",
      "   0.15996607]]\n"
     ]
    }
   ],
   "source": [
    "bert_embeddings = get_bert_embeddings_batched(list(essays[\"TEXT\"]))\n",
    "print(bert_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Feature Vector\n",
    "\n",
    "Combine features into a final feature vector, and separate targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.         ...  0.17866525 -0.13878608\n",
      "   0.08554857]\n",
      " [ 0.          0.          0.         ...  0.15697885 -0.10536592\n",
      "   0.07854819]\n",
      " [ 0.          0.          0.         ...  0.16525042 -0.14665474\n",
      "   0.08982378]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.15260881 -0.12059193\n",
      "   0.06254157]\n",
      " [ 0.          0.          0.         ...  0.147249   -0.13642329\n",
      "   0.10019936]\n",
      " [ 0.          0.          0.         ...  0.18132985 -0.1229706\n",
      "   0.09198   ]]\n",
      "[[0 1 1 0 1]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 1 1]\n",
      " ...\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 0 1]\n",
      " [0 1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "features = np.hstack([tfidf_features, sentiment_vectors, glove_embeddings])\n",
    "targets = (essays[[\"cEXT\", \"cNEU\", \"cAGR\", \"cCON\", \"cOPN\"]].values == \"y\").astype(int)\n",
    "\n",
    "print(features)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "Perform PCA to bring down the (very high) dimension of the current feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1198,
   "metadata": {},
   "outputs": [],
   "source": [
    "if features.shape[1] > 256:\n",
    "\tpca = PCA(n_components=256)\n",
    "\tfeatures = pca.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "### Test Train Split\n",
    "\n",
    "Create a 20-80 test-train split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1199,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1192,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Neural Network\n",
    "\n",
    "Build a fully-connected feed-forward neural network with 5 hidden layers, and L2 regularization with strength 0.001. Define a neuron dropout fraction of 0.4 per layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1193,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "\ttf.keras.layers.InputLayer(input_shape=(X_train.shape[1],)),\n",
    "\t# tf.keras.layers.Dense(512, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "\t# tf.keras.layers.BatchNormalization(),\n",
    "\t# tf.keras.layers.Dropout(0.4),\n",
    "\ttf.keras.layers.Dense(256, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "\ttf.keras.layers.BatchNormalization(),\n",
    "\ttf.keras.layers.Dropout(0.4),\n",
    "\ttf.keras.layers.Dense(128, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "\ttf.keras.layers.BatchNormalization(),\n",
    "\ttf.keras.layers.Dropout(0.4),\n",
    "\ttf.keras.layers.Dense(64, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "\ttf.keras.layers.BatchNormalization(),\n",
    "\ttf.keras.layers.Dropout(0.4),\n",
    "\ttf.keras.layers.Dense(32, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "\ttf.keras.layers.BatchNormalization(),\n",
    "\ttf.keras.layers.Dropout(0.4),\n",
    "\ttf.keras.layers.Dense(16, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "\ttf.keras.layers.BatchNormalization(),\n",
    "\ttf.keras.layers.Dropout(0.4),\n",
    "\ttf.keras.layers.Dense(y_train.shape[1], activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Define some functions to evaluate the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_loss(y_true, y_pred):\n",
    "    total_labels = y_true.size\n",
    "    incorrect_labels = (y_true != y_pred).sum()\n",
    "    return incorrect_labels / total_labels\n",
    "\n",
    "def exact_match_accuracy(y_true, y_pred):\n",
    "    matches = (y_true == y_pred).all(axis=1).sum()\n",
    "    total_samples = y_true.shape[0]\n",
    "    return matches / total_samples\n",
    "\n",
    "def label_wise_accuracy(y_true, y_pred):\n",
    "    per_label_accuracy = (y_true == y_pred).mean(axis=0)\n",
    "    return per_label_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Define an early stopping callback, and train the model ten times, evaluate exact match accuracy for each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 850us/step\n",
      "0.042682926829268296\n",
      "16/16 [==============================] - 0s 844us/step\n",
      "0.04878048780487805\n",
      "16/16 [==============================] - 0s 917us/step\n",
      "0.06707317073170732\n",
      "16/16 [==============================] - 0s 861us/step\n",
      "0.06504065040650407\n",
      "16/16 [==============================] - 0s 896us/step\n",
      "0.07113821138211382\n",
      "16/16 [==============================] - 0s 978us/step\n",
      "0.07723577235772358\n",
      "16/16 [==============================] - 0s 920us/step\n",
      "0.07113821138211382\n",
      "16/16 [==============================] - 0s 807us/step\n",
      "0.07520325203252033\n",
      "16/16 [==============================] - 0s 888us/step\n",
      "0.07520325203252033\n",
      "16/16 [==============================] - 0s 863us/step\n",
      "0.09146341463414634\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "\tmodel.fit(\n",
    "\t\tX_train, y_train, \n",
    "\t\tepochs=50, \n",
    "\t\tvalidation_split=0.2, \n",
    "\t\tcallbacks=[early_stopping],\n",
    "\t\tverbose=0\n",
    "\t)\n",
    "\n",
    "\ty_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "\t\n",
    "\tprint(exact_match_accuracy(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the last trained model in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming Loss: 0.44715447154471544\n",
      "Exact Match Accuracy: 0.09146341463414634\n",
      "Label-Wise Accuracy: [0.58130081 0.53861789 0.51422764 0.54471545 0.58536585]\n"
     ]
    }
   ],
   "source": [
    "print(\"Hamming Loss:\", hamming_loss(y_test, y_pred))\n",
    "print(\"Exact Match Accuracy:\", exact_match_accuracy(y_test, y_pred))\n",
    "print(\"Label-Wise Accuracy:\", label_wise_accuracy(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
